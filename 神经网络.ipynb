{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的神经元：房价预测\n",
    "\n",
    "<img src=\"images/最简单的神经元.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么用神经网络\n",
    "\n",
    "<img src=\"images/为什么用神经网络.png\" width=\"300\"/>    \n",
    "\n",
    "数据集越大，对越大的神经网络的性能越好。   \n",
    "对于比较小的数据集，各种模型的性能差不多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分 ： 二分类神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （一）样品的数据结构："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一张已经被标记为“xxx”的图 被称作样品\n",
    "\n",
    "<img src=\"images/三色图.png\" alt=\"image1\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像由形状$（长度，高度，深度=3）$的3D阵列表示。然而，当您读取图像作为算法的输入时，您可以将其转换为形状为$（长度*高度*3，1）$的向量。    \n",
    "换句话说，可以将三维阵列“展开”或重塑为一维矢量。\n",
    "也就是把3个图层的数据连起来排成一列，成为一个  $x$    \n",
    "具体方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "\n",
    "    a=image.shape[0]\n",
    "    b=image.shape[1]\n",
    "    c=image.shape[2]\n",
    "    v=image.reshape(a*b*c,1)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理好图像后，得到向量x作为输入变量     \n",
    "\n",
    "**输入**  \n",
    "$x$: 表示一个 $n_x$ 维度特征，为输入特征，维度为 $(n_x, 1)$。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**输出**  \n",
    "$y$: 表示输出结果，取值为 $(0,1)$;  \n",
    "$(x^{(i)}, y^{(i)})$: 表示第 $i$ 组数据，可能是训练数据，也可能是测试数据，此处暂译为训练数据;\n",
    "\n",
    "---\n",
    "\n",
    "$X = [x^{(1)}, x^{(2)}, \\dots, x^{(m)}]$: 表示不同的训练数据组成的输入矩阵，放在一个 $n_x \\times m$ 的矩阵中;  \n",
    "$Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}]$: 对应训练不同训练数据组成的输出矩阵，维度为 $1 \\times m$。\n",
    "\n",
    "表示测试集的时候，我们会用 $M$ 来默认表示 $𝑀_{train}$  ,而测试集  $𝑀_{test}$ 需要单独注明：   \n",
    "\n",
    "$M = [(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)})], \\dots, (x^{(m)}, y^{(m)})]  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）逻辑回归（Logistic Regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二元分类问题来说，给定一个输入特征向量 $x$，它可能属于一类或另一类，模型的任务就是找出其属于哪一类。   \n",
    "我们的模型在计算过程中，需要将输入特征 $x$ 转换为输出估计值 $\\hat{y}$。    \n",
    "比如对于猫图而言，如果“是猫图”的 $y$ 表示为 1， “不是猫图”的 $y$ 表示为 0   \n",
    "那么  $\\hat{y}$  需要在（0,1）之内，表示“是猫图”的可能性\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始之前，我们先介绍一下用sigmoid函数来处理向量x\n",
    "\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$\n",
    "\n",
    "sigmoid函数的导数为：\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x))\n",
    "    return s\n",
    "def sigmoid_derivative(x):\n",
    "    ds=sigmoid(x)*(1-sigmoid(x))    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是预测的模型\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\sigma(w^T x^{(i)} + b), \\text{ where } \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Given $\\{(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})\\}$, 而我们希望 $\\hat{y}^{(i)} \\approx y^{(i)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何来衡量模型的准确性呢？  \n",
    "需要用到 “损失函数”（loss function）：\n",
    "\n",
    "$$ \n",
    "L(\\hat{y},y) = -ylog(\\hat{y})-(1-y)log(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "当 y=1 时，只有 $\\hat{y}$ 尽可能大（收范围限制趋近于1），损失函数L才会小   \n",
    "当 y=0 时，只有 $\\hat{y}$ 尽可能小（收范围限制趋近于0），损失函数L才会小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而这只是对于一个样本的衡量方法，对于一个样本集而言，需要进行累加，这称为 成本函数（Cost Function）\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即\n",
    "$$\n",
    "J(w, b)= \\frac{1}{m} \\sum_{i=1}^m \\left(-y^{(i)} \\log(\\hat{y}^{(i)}) - (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么现在优化的方向就已经相当明确了，我们要不断修改w和b来让成本函数尽量小   \n",
    "而事实上，使用了sigmoid 函数，这个成本函数是有最低点的 \n",
    "\n",
    "\n",
    "\n",
    "<img src='images/成本函数3D.png'><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以直接使用 **梯度下降法**：\n",
    "\n",
    "假设b不变时：\n",
    "$$\n",
    "w = w - a \\frac{dJ(w)}{dw}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "其中      \n",
    "𝑎 ：学习率（ learning rate）    \n",
    "$a \\frac{dJ(w)}{dw}$ : 步长 (step），即向下走一步的长度\n",
    "\n",
    "注意：   \n",
    "对于 $\\frac{dJ(w)}{dw}$  我们一般简写做 $dw$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓展到两个参数就是：\n",
    "$$\n",
    "w := w - a \\frac{\\partial J(w, b)}{\\partial w}, \\quad b := b - a \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）逻辑回归中的梯度下降（Logistic Regression Gradient Descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在前面已经了解了逻辑回归的训练过程    \n",
    "\n",
    "那么怎么计算 $\\frac{\\partial J(w, b)}{\\partial w}$  和  $\\frac{\\partial J(w, b)}{\\partial b}$   呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设样本有2个特征 $x_1,x_2$，那么 z 的表达式应该修改为：\n",
    "\n",
    "$$\n",
    "z=w_1x_1+w_2x_2+b\n",
    "$$\n",
    "\n",
    "\n",
    "回忆一下：   \n",
    "$\\hat{y}^{(i)} = \\sigma(w^T x^{(i)} + b) = a, \\text{ where } \\sigma(z) = \\frac{1}{1+e^{-z}}$  \n",
    "\n",
    "其损失函数为：   \n",
    "$L(\\hat{y},y) = -ylog(\\hat{y})-(1-y)log(1-\\hat{y})$    \n",
    "\n",
    "对于单个样本而言，代价函数 $J(w,b) $ 就是损失函数：   \n",
    "$L(a,y) = -ylog(a)-(1-y)log(1-a)$    \n",
    "\n",
    "其中𝑎是逻辑回归的输出， 𝑦是样本的标签值\n",
    "\n",
    "求导可得：   \n",
    "$$ \\frac{dL(a,y)}{da} = -\\frac{y}{a}+\\frac{1-y}{1-a} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时：\n",
    "$$\n",
    "\\begin{align*}\n",
    "a &= \\sigma(z) = \\frac{1}{1+e^{-z}} \\\\\n",
    "\\implies 1 + e^{-z} &= \\frac{1}{a} \\\\\n",
    "\\implies -e^{-z} \\, dz &= -\\frac{1}{a^2} \\, da \\\\\n",
    "\\implies \\frac{da}{dz} &= a^2 e^{-z} \\\\\n",
    "&= a^2 \\left( \\frac{1}{a} - 1 \\right) \\\\\n",
    "&= a(1-a)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为：   \n",
    "$$ \n",
    "\\frac{dL}{dz}  =\\frac{dL}{da} \\frac{da}{dz}     \\\\\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{dz} =(-\\frac{y}{a}+\\frac{1-y}{1-a}) *  a(1-a)\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{dz}= a-y\n",
    "$$\n",
    "\n",
    "同时，我们一般直接用 $dz$ 来表示 $\\frac{dL}{dz}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为：\n",
    "$$ \n",
    "\\frac{dL}{dw}  =\\frac{dL}{dz} \\frac{dz}{dw}     \\\\\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{dw} =(a-y)(x)\n",
    "$$\n",
    "\n",
    "所以：   \n",
    "$\\frac{dL}{dw_1} =(a-y)(x_1)$    \n",
    "\n",
    "$\\frac{dL}{dw_2} =(a-y)(x_2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为：\n",
    "$$ \n",
    "\\frac{dL}{db}  =\\frac{dL}{dz} \\frac{dz}{db}     \\\\\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{db} =a-y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，对于单个样本的梯度下降算法：    \n",
    "$$\n",
    "w_1=w_1-adw_1  \\\\\n",
    "w_2=w_2-adw_2   \\\\\n",
    "b=b-adb\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "dw_1 =(a-y)(x_1) \\\\\n",
    "dw_2 =(a-y)(x_2)\\\\\n",
    "db=a-y\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结论拓展到m个样本应该如何呢？     \n",
    "我们知道：\n",
    "$J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})  $     \n",
    "即：  \n",
    "$J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(a^{(i)}, y^{(i)})$    \n",
    "其中 $a^{(i)}$ 是第 i 个样本的预测输出，$y^{(i)}$是第 i 个样本的实际标签。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "对权重 \\(w_1\\) 的梯度：   \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_1} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)}) x_1^{(i)}\n",
    "$$\n",
    "\n",
    "对权重 \\(w_2\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_2} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)}) x_2^{(i)}\n",
    "$$\n",
    "\n",
    "对偏置 \\(b\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "梯度下降更新规则\n",
    "使用上述计算出的平均梯度来更新参数：\n",
    "$$\n",
    "w_1 = w_1 - \\alpha \\frac{\\partial J}{\\partial w_1} \\\\\n",
    "w_2 = w_2 - \\alpha \\frac{\\partial J}{\\partial w_2} \\\\\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，用于控制更新步骤的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （四）向量化（Vectorization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的计算方法中,每进行一次梯度下降时，都需要使用for循环来遍历每一个样本    \n",
    "事实上这样的效率非常低，所以我们学习使用 **向量化** 来解决这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓向量化，就是对于一串数据，比如数组或者矩阵，本来采用for循环进行一个个的运算，    \n",
    "但是现在使用numpy库的自带功能，去掉显式的for循环     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized computation time: 0.0019941329956054688\n",
      "[0.88992777 0.75336519 0.87875718 0.93422771 0.88558512]\n",
      "Loop computation time: 0.12228894233703613\n",
      "[0.88992777 0.75336519 0.87875718 0.93422771 0.88558512]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "data = np.random.rand(1000000)  # 假设有一个大型数组\n",
    "\n",
    "# 向量化计算平方根和平均值\n",
    "tik = time.time()\n",
    "sqrt_data_vectorized = np.sqrt(data)\n",
    "tok = time.time()\n",
    "time1 = tok - tik\n",
    "print('time:', time1)\n",
    "print(sqrt_data_vectorized[:5])  # 打印前5个元素作为示例\n",
    "\n",
    "\n",
    "# 普通计算方法\n",
    "tik = time.time()\n",
    "sqrt_list = []  # 使用列表收集平方根结果\n",
    "for i in data:\n",
    "    sqrt_list.append(i**0.5)  # 向列表追加平方根\n",
    "sqrt_data = np.array(sqrt_list)  # 将列表转换为NumPy数组\n",
    "tok = time.time()\n",
    "time2 = tok - tik    \n",
    "print('time:', time2)\n",
    "print(sqrt_data[:5])  # 打印前5个元素作为示例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的结果可以看到相差了100多倍    \n",
    "注意：  \n",
    "所谓向量化，不一定要调用numpy的计算函数比如 np.mean() np.sqrt()等，而是使用了np.array 的数据格式   \n",
    "比如对于一个 np.array 进行 **2 平方操作，也是向量化的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据我们已经学会的算法，可以知道现在的计算过程如下：   \n",
    "\n",
    "初始化 $J=0$, $dw_1=0$, $dw_2=0$, $db=0$。代码流程如下：\n",
    "\n",
    "```python\n",
    "J = 0; dw1 = 0; dw2 = 0; db = 0\n",
    "for i in range(1, m+1):  # 假设m是样本数量\n",
    "    z_i = w * x[i] + b  # 这里假设x[i]是一个包含x1和x2的向量\n",
    "    a_i = sigmoid(z_i)\n",
    "    J += -[y[i] * log(a_i) + (1 - y[i]) * log(1 - a_i)]\n",
    "    dz_i = a_i - y[i]\n",
    "    dw1 += x[i][0] * dz_i  # 假设x[i][0]是特征x1\n",
    "    dw2 += x[i][1] * dz_i  # 假设x[i][1]是特征x2\n",
    "    db += dz_i\n",
    "\n",
    "# 外部循环结束后，计算平均值\n",
    "J /= m\n",
    "dw1 /= m\n",
    "dw2 /= m\n",
    "db /= m\n",
    "\n",
    "# 更新参数\n",
    "w1 = w1 - alpha * dw1\n",
    "w2 = w2 - alpha * dw2\n",
    "b = b - alpha * db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这段代码中实现了一次梯度下降，也就是一次训练，但是使用了两个循环     \n",
    "第一个循环是for循环遍历每一个样本     \n",
    "第二个循环是对特征值进行循环。在这例子我们有 2 个特征值。如果你有超过两个特征时，需要循环 𝑑𝑤1 、 𝑑𝑤2 、 𝑑𝑤3 等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们先来看**第二个循环的向量化**      \n",
    "不用初始化 𝑑𝑤1 𝑑𝑤2 都等于 0，而是定义 𝑑𝑤 为一个向量，设置 $ dw=np.zeros(n_x,1)  $  定义一个x行的一位向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/第一层向量化.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看 **第二个循环的向量化**     \n",
    "回忆一下最开始讲到的输入集\n",
    "\n",
    "\n",
    "$X = [x^{(1)}, x^{(2)}, \\dots, x^{(m)}]$: 表示不同的训练数据组成的输入矩阵，放在一个 $n_x \\times m$ 的矩阵中;  \n",
    "$Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}]$: 对应训练不同训练数据组成的输出矩阵，维度为 $1 \\times m$。\n",
    "\n",
    "表示测试集的时候，我们会用 $M$ 来默认表示 $𝑀_{train}$  ,而测试集  $𝑀_{test}$ 需要单独注明：   \n",
    "\n",
    "$M = [(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)})], \\dots, (x^{(m)}, y^{(m)})]  $\n",
    "\n",
    "\n",
    "\n",
    "所以先计算 $z_1,z_2,z_3, \\dots, z_n$  ,把他们都放到一个  $1 \\times m$ 的行向量中    \n",
    "你可以发现他可以表达为  $w^T$  (w的转置) $\\times X+[b,b,\\dots,b]$    \n",
    "$[b,b,\\dots,b]$ 是一个 $1 \\times m$ 的行向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以计算的最终得到的$Z$是一个 $1 \\times m$ 的向量，$Z = \\begin{bmatrix} z^{(1)} & z^{(2)} & \\ldots & z^{(m)} \\end{bmatrix}$，计算方式为 $z = w^T X + \\mathbf{b}$，其中 $\\mathbf{b} = \\begin{bmatrix} b & b & \\ldots & b \\end{bmatrix}$   \n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "w^T x^{(1)} + b \\\\\n",
    "w^T x^{(2)} + b \\\\\n",
    "\\vdots \\\\\n",
    "w^T x^{(m)} + b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其中，\n",
    "- $w^T x^{(1)} + b$ 是向量 $Z$ 的第一个元素，\n",
    "- $w^T x^{(2)} + b$ 是第二个元素，\n",
    "- 以此类推，直到 $w^T x^{(m)} + b$ 是第 $m$ 个元素。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=np.dot(w.T, X)+b\n",
    "# 其中b通过广播机制自动被拓展成一个 $1 \\times m$ 的行向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加下来要使用向量Z计算出向量Y    \n",
    "$Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}]$: 对应训练不同训练数据组成的输出矩阵，维度为 $1 \\times m$。\n",
    "\n",
    "然后就可以计算 $ dZ =A-Y=[a^{(1)}-y^{(1)},a^{(2)}-y^{(2)},\\dots, a^{(n)}-y^{(n)}   ]  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对偏置 \\(b\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)})\n",
    "$$\n",
    "所以 $ db=\\frac{1}{m}*\\sum_{i=1}^m (dz^{(i)}) $   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=(1/m)*np.sum(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对权重 \\(w\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)}) x^{(i)}\n",
    "$$\n",
    "\n",
    "所以  $ db=\\frac{1}{m}*X*dz^T $   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (五)总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是没有使用向量化的计算过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/原始计算过程.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是使用向量化之后的计算过程   \n",
    "\n",
    "\n",
    "<img src='images/两层向量化.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (六)注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里要讲一个重要且常见的bug——一维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.random.rand(5)\n",
    "print(a.shape)  #(5,)\n",
    "# 此时a就是一个一维数组，既不是行向量也不是列向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种结构很容易出现意想不到的bug，所以是要坚决摈弃的，一定要修改成(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.random.rand(5,1)\n",
    "print(a.shape)  #(5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分：浅层神经网络（Shallow neural networks）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
