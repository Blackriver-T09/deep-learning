{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的神经元：房价预测\n",
    "\n",
    "<img src=\"images/最简单的神经元.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么用神经网络\n",
    "\n",
    "<img src=\"images/为什么用神经网络.png\" width=\"300\"/>    \n",
    "\n",
    "数据集越大，对越大的神经网络的性能越好。   \n",
    "对于比较小的数据集，各种模型的性能差不多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分 ： 二分类神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （一）样品的数据结构："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一张已经被标记为“xxx”的图 被称作样品\n",
    "\n",
    "<img src=\"images/三色图.png\" alt=\"image1\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像由形状$（长度，高度，深度=3）$的3D阵列表示。然而，当您读取图像作为算法的输入时，您可以将其转换为形状为$（长度*高度*3，1）$的向量。    \n",
    "换句话说，可以将三维阵列“展开”或重塑为一维矢量。\n",
    "也就是把3个图层的数据连起来排成一列，成为一个  $x$    \n",
    "具体方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "\n",
    "    a=image.shape[0]\n",
    "    b=image.shape[1]\n",
    "    c=image.shape[2]\n",
    "    v=image.reshape(a*b*c,1)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理好图像后，得到向量x作为输入变量     \n",
    "\n",
    "**输入**  \n",
    "$x$: 表示一个 $n_x$ 维度特征，为输入特征，维度为 $(n_x, 1)$。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**输出**  \n",
    "$y$: 表示输出结果，取值为 $(0,1)$;  \n",
    "$(x^{(i)}, y^{(i)})$: 表示第 $i$ 组数据，可能是训练数据，也可能是测试数据，此处暂译为训练数据;\n",
    "\n",
    "---\n",
    "\n",
    "$X = [x^{(1)}, x^{(2)}, \\dots, x^{(m)}]$: 表示不同的训练数据组成的输入矩阵，放在一个 $n_x \\times m$ 的矩阵中;  \n",
    "$Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}]$: 对应训练不同训练数据组成的输出矩阵，维度为 $1 \\times m$。\n",
    "\n",
    "表示测试集的时候，我们会用 $M$ 来默认表示 $𝑀_{train}$  ,而测试集  $𝑀_{test}$ 需要单独注明：   \n",
    "\n",
    "$M = [(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)})], \\dots, (x^{(m)}, y^{(m)})]  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）逻辑回归（Logistic Regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二元分类问题来说，给定一个输入特征向量 $x$，它可能属于一类或另一类，模型的任务就是找出其属于哪一类。   \n",
    "我们的模型在计算过程中，需要将输入特征 $x$ 转换为输出估计值 $\\hat{y}$。    \n",
    "比如对于猫图而言，如果“是猫图”的 $y$ 表示为 1， “不是猫图”的 $y$ 表示为 0   \n",
    "那么  $\\hat{y}$  需要在（0,1）之内，表示“是猫图”的可能性\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始之前，我们先介绍一下用sigmoid函数来处理向量x\n",
    "\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$\n",
    "\n",
    "sigmoid函数的导数为：\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x))\n",
    "    return s\n",
    "def sigmoid_derivative(x):\n",
    "    ds=sigmoid(x)*(1-sigmoid(x))    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是预测的模型\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\sigma(w^T x^{(i)} + b), \\text{ where } \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Given $\\{(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})\\}$, 而我们希望 $\\hat{y}^{(i)} \\approx y^{(i)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何来衡量模型的准确性呢？  \n",
    "需要用到 “损失函数”（loss function）：\n",
    "\n",
    "$$ \n",
    "L(\\hat{y},y) = -ylog(\\hat{y})-(1-y)log(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "当 y=1 时，只有 $\\hat{y}$ 尽可能大（收范围限制趋近于1），损失函数L才会小   \n",
    "当 y=0 时，只有 $\\hat{y}$ 尽可能小（收范围限制趋近于0），损失函数L才会小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而这只是对于一个样本的衡量方法，对于一个样本集而言，需要进行累加，这称为 成本函数（Cost Function）\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即\n",
    "$$\n",
    "J(w, b)= \\frac{1}{m} \\sum_{i=1}^m \\left(-y^{(i)} \\log(\\hat{y}^{(i)}) - (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么现在优化的方向就已经相当明确了，我们要不断修改w和b来让成本函数尽量小   \n",
    "而事实上，使用了sigmoid 函数，这个成本函数是有最低点的 \n",
    "\n",
    "\n",
    "\n",
    "<img src='images/成本函数3D.png'><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以直接使用 **梯度下降法**：\n",
    "\n",
    "假设b不变时：\n",
    "$$\n",
    "w = w - a \\frac{dJ(w)}{dw}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "其中      \n",
    "𝑎 ：学习率（ learning rate）    \n",
    "$a \\frac{dJ(w)}{dw}$ : 步长 (step），即向下走一步的长度\n",
    "\n",
    "注意：   \n",
    "对于 $\\frac{dJ(w)}{dw}$  我们一般简写做 $dw$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓展到两个参数就是：\n",
    "$$\n",
    "w := w - a \\frac{\\partial J(w, b)}{\\partial w}, \\quad b := b - a \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）逻辑回归中的梯度下降（Logistic Regression Gradient Descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在前面已经了解了逻辑回归的训练过程    \n",
    "\n",
    "那么怎么计算 $\\frac{\\partial J(w, b)}{\\partial w}$  和  $\\frac{\\partial J(w, b)}{\\partial b}$   呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设样本有2个特征 $x_1,x_2$，那么 z 的表达式应该修改为：\n",
    "\n",
    "$$\n",
    "z=w_1x_1+w_2x_2+b\n",
    "$$\n",
    "\n",
    "\n",
    "回忆一下：   \n",
    "$\\hat{y}^{(i)} = \\sigma(w^T x^{(i)} + b) = a, \\text{ where } \\sigma(z) = \\frac{1}{1+e^{-z}}$  \n",
    "\n",
    "其损失函数为：   \n",
    "$L(\\hat{y},y) = -ylog(\\hat{y})-(1-y)log(1-\\hat{y})$    \n",
    "\n",
    "对于单个样本而言，代价函数 $J(w,b) $ 就是损失函数：   \n",
    "$L(a,y) = -ylog(a)-(1-y)log(1-a)$    \n",
    "\n",
    "其中𝑎是逻辑回归的输出， 𝑦是样本的标签值\n",
    "\n",
    "求导可得：   \n",
    "$$ \\frac{dL(a,y)}{da} = -\\frac{y}{a}+\\frac{1-y}{1-a} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时：\n",
    "$$\n",
    "\\begin{align*}\n",
    "a &= \\sigma(z) = \\frac{1}{1+e^{-z}} \\\\\n",
    "\\implies 1 + e^{-z} &= \\frac{1}{a} \\\\\n",
    "\\implies -e^{-z} \\, dz &= -\\frac{1}{a^2} \\, da \\\\\n",
    "\\implies \\frac{da}{dz} &= a^2 e^{-z} \\\\\n",
    "&= a^2 \\left( \\frac{1}{a} - 1 \\right) \\\\\n",
    "&= a(1-a)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为：   \n",
    "$$ \n",
    "\\frac{dL}{dz}  =\\frac{dL}{da} \\frac{da}{dz}     \\\\\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{dz} =(-\\frac{y}{a}+\\frac{1-y}{1-a}) *  a(1-a)\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{dz}= a-y\n",
    "$$\n",
    "\n",
    "同时，我们一般直接用 $dz$ 来表示 $\\frac{dL}{dz}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为：\n",
    "$$ \n",
    "\\frac{dL}{dw}  =\\frac{dL}{dz} \\frac{dz}{dw}     \\\\\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{dw} =(a-y)(x)\n",
    "$$\n",
    "\n",
    "所以：   \n",
    "$\\frac{dL}{dw_1} =(a-y)(x_1)$    \n",
    "\n",
    "$\\frac{dL}{dw_2} =(a-y)(x_2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为：\n",
    "$$ \n",
    "\\frac{dL}{db}  =\\frac{dL}{dz} \\frac{dz}{db}     \\\\\n",
    "$$\n",
    "$$\n",
    "\\implies \\frac{dL}{db} =a-y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，对于单个样本的梯度下降算法：    \n",
    "$$\n",
    "w_1=w_1-adw_1  \\\\\n",
    "w_2=w_2-adw_2   \\\\\n",
    "b=b-adb\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "dw_1 =(a-y)(x_1) \\\\\n",
    "dw_2 =(a-y)(x_2)\\\\\n",
    "db=a-y\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结论拓展到m个样本应该如何呢？     \n",
    "我们知道：\n",
    "$J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})  $     \n",
    "即：  \n",
    "$J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(a^{(i)}, y^{(i)})$    \n",
    "其中 $a^{(i)}$ 是第 i 个样本的预测输出，$y^{(i)}$是第 i 个样本的实际标签。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "对权重 \\(w_1\\) 的梯度：   \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_1} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)}) x_1^{(i)}\n",
    "$$\n",
    "\n",
    "对权重 \\(w_2\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_2} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)}) x_2^{(i)}\n",
    "$$\n",
    "\n",
    "对偏置 \\(b\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "梯度下降更新规则\n",
    "使用上述计算出的平均梯度来更新参数：\n",
    "$$\n",
    "w_1 = w_1 - \\alpha \\frac{\\partial J}{\\partial w_1} \\\\\n",
    "w_2 = w_2 - \\alpha \\frac{\\partial J}{\\partial w_2} \\\\\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，用于控制更新步骤的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （四）向量化（Vectorization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的计算方法中,每进行一次梯度下降时，都需要使用for循环来遍历每一个样本    \n",
    "事实上这样的效率非常低，所以我们学习使用 **向量化** 来解决这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓向量化，就是对于一串数据，比如数组或者矩阵，本来采用for循环进行一个个的运算，    \n",
    "但是现在使用numpy库的自带功能，去掉显式的for循环     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized computation time: 0.0019941329956054688\n",
      "[0.88992777 0.75336519 0.87875718 0.93422771 0.88558512]\n",
      "Loop computation time: 0.12228894233703613\n",
      "[0.88992777 0.75336519 0.87875718 0.93422771 0.88558512]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "data = np.random.rand(1000000)  # 假设有一个大型数组\n",
    "\n",
    "# 向量化计算平方根和平均值\n",
    "tik = time.time()\n",
    "sqrt_data_vectorized = np.sqrt(data)\n",
    "tok = time.time()\n",
    "time1 = tok - tik\n",
    "print('time:', time1)\n",
    "print(sqrt_data_vectorized[:5])  # 打印前5个元素作为示例\n",
    "\n",
    "\n",
    "# 普通计算方法\n",
    "tik = time.time()\n",
    "sqrt_list = []  # 使用列表收集平方根结果\n",
    "for i in data:\n",
    "    sqrt_list.append(i**0.5)  # 向列表追加平方根\n",
    "sqrt_data = np.array(sqrt_list)  # 将列表转换为NumPy数组\n",
    "tok = time.time()\n",
    "time2 = tok - tik    \n",
    "print('time:', time2)\n",
    "print(sqrt_data[:5])  # 打印前5个元素作为示例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的结果可以看到相差了100多倍    \n",
    "注意：  \n",
    "所谓向量化，不一定要调用numpy的计算函数比如 np.mean() np.sqrt()等，而是使用了np.array 的数据格式   \n",
    "比如对于一个 np.array 进行 **2 平方操作，也是向量化的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据我们已经学会的算法，可以知道现在的计算过程如下：   \n",
    "\n",
    "初始化 $J=0$, $dw_1=0$, $dw_2=0$, $db=0$。代码流程如下：\n",
    "\n",
    "```python\n",
    "J = 0; dw1 = 0; dw2 = 0; db = 0\n",
    "for i in range(1, m+1):  # 假设m是样本数量\n",
    "    z_i = w * x[i] + b  # 这里假设x[i]是一个包含x1和x2的向量\n",
    "    a_i = sigmoid(z_i)\n",
    "    J += -[y[i] * log(a_i) + (1 - y[i]) * log(1 - a_i)]\n",
    "    dz_i = a_i - y[i]\n",
    "    dw1 += x[i][0] * dz_i  # 假设x[i][0]是特征x1\n",
    "    dw2 += x[i][1] * dz_i  # 假设x[i][1]是特征x2\n",
    "    db += dz_i\n",
    "\n",
    "# 外部循环结束后，计算平均值\n",
    "J /= m\n",
    "dw1 /= m\n",
    "dw2 /= m\n",
    "db /= m\n",
    "\n",
    "# 更新参数\n",
    "w1 = w1 - alpha * dw1\n",
    "w2 = w2 - alpha * dw2\n",
    "b = b - alpha * db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这段代码中实现了一次梯度下降，也就是一次训练，但是使用了两个循环     \n",
    "第一个循环是for循环遍历每一个样本     \n",
    "第二个循环是对特征值进行循环。在这例子我们有 2 个特征值。如果你有超过两个特征时，需要循环 𝑑𝑤1 、 𝑑𝑤2 、 𝑑𝑤3 等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们先来看**第二个循环的向量化**      \n",
    "不用初始化 𝑑𝑤1 𝑑𝑤2 都等于 0，而是定义 𝑑𝑤 为一个向量，设置 $ w=np.zeros(n_x,1)  $  定义一个$n_x$行的一维向量    \n",
    "其中 $n_x $代表单个样本的特征数，对于一张图片而言可能是64* 64 *3    \n",
    "而其实dw不需要专门在代码开头初始化，因为dw是根据w自动算出来的，形状也是取决于w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/第一层向量化.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看 **第一个循环的向量化**     \n",
    "回忆一下最开始讲到的输入集\n",
    "\n",
    "\n",
    "$X = [x^{(1)}, x^{(2)}, \\dots, x^{(m)}]$: 表示不同的训练数据组成的输入矩阵，放在一个 $n_x \\times m$ 的矩阵中;  \n",
    "$n_x$ 是单个样本的特征点数，对于图片而言可能是64* 64 *3\n",
    "\n",
    "$Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}]$: 对应训练不同训练数据组成的输出矩阵，维度为 $1 \\times m$。\n",
    "\n",
    "表示测试集的时候，我们会用 $M$ 来默认表示 $𝑀_{train}$  ,而测试集  $𝑀_{test}$ 需要单独注明：   \n",
    "\n",
    "$M = [(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)})], \\dots, (x^{(m)}, y^{(m)})]  $\n",
    "\n",
    "\n",
    "\n",
    "所以先计算 $z_1,z_2,z_3, \\dots, z_n$  ,把他们都放到一个  $1 \\times m$ 的行向量中    \n",
    "你可以发现他可以表达为  $w^T$  (w的转置) $\\times X+[b,b,\\dots,b]$    \n",
    "$[b,b,\\dots,b]$ 是一个 $1 \\times m$ 的行向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以计算的最终得到的$Z$是一个 $1 \\times m$ 的向量，$Z = \\begin{bmatrix} z^{(1)} & z^{(2)} & \\ldots & z^{(m)} \\end{bmatrix}$，计算方式为 $z = w^T X + \\mathbf{b}$，其中 $\\mathbf{b} = \\begin{bmatrix} b & b & \\ldots & b \\end{bmatrix}$   \n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "w^T x^{(1)} + b \\\\\n",
    "w^T x^{(2)} + b \\\\\n",
    "\\vdots \\\\\n",
    "w^T x^{(m)} + b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其中，\n",
    "- $w^T x^{(1)} + b$ 是向量 $Z$ 的第一个元素，\n",
    "- $w^T x^{(2)} + b$ 是第二个元素，\n",
    "- 以此类推，直到 $w^T x^{(m)} + b$ 是第 $m$ 个元素。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=np.dot(w.T, X)+b\n",
    "# 其中b通过广播机制自动被拓展成一个 $1 \\times m$ 的行向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加下来要使用向量Z计算出向量Y    \n",
    "$Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}]$: 对应训练不同训练数据组成的输出矩阵，维度为 $1 \\times m$。\n",
    "\n",
    "然后就可以计算 $ dZ =A-Y=[a^{(1)}-y^{(1)},a^{(2)}-y^{(2)},\\dots, a^{(n)}-y^{(n)}   ]  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对偏置 \\(b\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)})\n",
    "$$\n",
    "所以 $ db=\\frac{1}{m}*\\sum_{i=1}^m (dz^{(i)}) $   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=(1/m)*np.sum(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对权重 \\(w\\) 的梯度：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)} - y^{(i)}) x^{(i)}\n",
    "$$\n",
    "\n",
    "所以  $ db=\\frac{1}{m}*X*dz^T $   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (五)总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是没有使用向量化的计算过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/原始计算过程.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是使用向量化之后的计算过程   \n",
    "\n",
    "\n",
    "<img src='images/两层向量化.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (六)注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里要讲一个重要且常见的bug——一维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.random.rand(5)\n",
    "print(a.shape)  #(5,)\n",
    "# 此时a就是一个一维数组，既不是行向量也不是列向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种结构很容易出现意想不到的bug，所以是要坚决摈弃的，一定要修改成(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.random.rand(5,1)\n",
    "print(a.shape)  #(5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分：浅层神经网络（Shallow neural networks）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （一）浅层神经网络介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一部分，我们学习了二分类神经网络     \n",
    "<img src='images/二分类神经网络.png'>\n",
    "\n",
    "中间这个节点的计算过程如下：\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\textbf{x} \\\\\n",
    "\\textbf{w} \\\\\n",
    "b \\\\\n",
    "\\end{array}\n",
    "\\rightarrow z = w^T x + b    \\\\\n",
    "\\rightarrow \\alpha = \\sigma(z)   \\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们将要学习浅层的神经网络   \n",
    "<img src='images/浅层神经网络.png'>    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "其中，我们用方框[1]表示第一层, [2]表示第二层，以此类推     \n",
    "其中 [0] 也称为输入层，最后的 [2] 也称为输出层，中间的都叫做隐藏层。      \n",
    "输入层是不算的，所以这一共有两层，所以叫“双层神经网络”    \n",
    "在浅层的神经网络这幅图中，每一个神经节点也都是执行相同的操作      \n",
    "\n",
    "以第一层的第一个节点为例\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\textbf{x} \\\\\n",
    "W^{[1]} \\\\\n",
    "b^{[1]} \\\\\n",
    "\\end{array}\n",
    "\\rightarrow z^{[1]} = W^{[1]} x + b^{[1]} \\rightarrow a^{[1]} = \\sigma(z^{[1]})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "但是正如图中所见，一层中是有多个神经元的    \n",
    "\n",
    "<img src='images/双层神经网络.png'>\n",
    "\n",
    "\n",
    "每一个节点最终都会输出一个预测值（或者叫激活值）a，就是我们在第一部分中的a    （ a 表示激活的意思）   \n",
    "为了普遍化，我们直接把每个节点（包括输入层和输出层）的最终值都称为 a      \n",
    "输入层的激活值称为 $a^{[0]}$，第一层的激活值记作 $a^{[1]}$。   \n",
    "第一层的第一个单元或结点我们将其表示为 $a_1^{[1]}$，第二个结点的值我们记为 $a_2^{[1]}$ 以此类推。\n",
    "\n",
    "\n",
    "$$\n",
    "a^{[1]} = \\begin{bmatrix}\n",
    "a_1^{[1]} \\\\\n",
    "a_2^{[1]} \\\\\n",
    "a_3^{[1]} \\\\\n",
    "a_4^{[1]}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "出于显而易见的向量化考虑，可以让$a^{[1]}$成为一个 $4 \\times 1$ 的矩阵，$a^{[2]}$成为一个 $1 \\times 1$ 的矩阵    \n",
    "注意：   \n",
    "**图中的 $x_1,x_2,x_3$ 等是指一个样本的不同特征,一共有$n_x$个，这里简化表达为3个，对于一张图片而言可能$n_x=64*64*3$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的道理，每层都有每层的W参数和b参数，    \n",
    "\n",
    "显而易见 $W^{[1]}$ 的形状是 $4 \\times 3$,  $b^{[1]}$ 的形状是 $4 \\times 1$    \n",
    "（4是因为本层有4个节点，每个节点都有一个w，3是因为每个w都要处理3个样本特征，所以是3）   \n",
    "\n",
    "同理可得 $W^{[2]}$ 的形状是 $1 \\times 4$,  $b^{[2]}$ 的形状是 $1 \\times 1$    \n",
    "（1是因为本层有1个节点，每个节点都有一个w，4是因为每个w都要处理4个上一隐藏层的a的输入，所以是4）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （二）浅层神经网络的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模仿二分类神经网络的计算，浅层神经网络的计算过程如下：       \n",
    "每个小圆圈代表了计算的两个主要概念。\n",
    "\n",
    "第一步，计算 $z_1^{[1]} = w_1^{[1]T} x + b_1^{[1]}$。\n",
    "\n",
    "第二步，通过激活函数计算 $a_1^{[1]} = \\sigma(z_1^{[1]})$。\n",
    "\n",
    "随着层的第一个以及后面的每个神经元的计算过程一样，只是注意每层表示不同，层级分别对应 $a_2^{[1]}, a_3^{[1]}, a_4^{[1]}$，详细过程见下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_1^{[1]} &= w_1^{[1]T} x + b_1^{[1]}, & a_1^{[1]} &= \\sigma(z_1^{[1]}) \\\\\n",
    "z_2^{[1]} &= w_2^{[1]T} x + b_2^{[1]}, & a_2^{[1]} &= \\sigma(z_2^{[1]}) \\\\\n",
    "z_3^{[1]} &= w_3^{[1]T} x + b_3^{[1]}, & a_3^{[1]} &= \\sigma(z_3^{[1]}) \\\\\n",
    "z_4^{[1]} &= w_4^{[1]T} x + b_4^{[1]}, & a_4^{[1]} &= \\sigma(z_4^{[1]})\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显而易见地可以使用向量化计算     \n",
    "$$\n",
    "Z^{[1]} = W^{[1]T} x + b^{[1]},  a^{[1]} = \\sigma(Z^{[1]})   \\\\\n",
    "\\\\\n",
    "Z^{[n]} = W^{[n]T} x + b^{[n]},  a^{[n]} = \\sigma(Z^{[n]})\n",
    "$$\n",
    "\n",
    "<img src='images/双层计算过程1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓展到整个过程，就是如图    \n",
    "<img src='images/双层计算过程2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，由于目前所有的计算都是针对单个样本的多个特征进行，所以还需要多个样本进行向量化      \n",
    "\n",
    "**注：** 例如 $a^{[2](i)}$  (i) 是第i个训练样本，[2]是第二层\n",
    "\n",
    "对于所有训练样本，需要让 𝑖 从 1到 𝑚实现下面这四个等式：\n",
    "\n",
    "**计算第一层神经元的输出**：\n",
    "- 计算线性组合 ：\n",
    "$$\n",
    "z^{[1](i)} = W^{[1]}(i) \\cdot x^{(i)} + b^{[1]}(i)\n",
    "$$\n",
    "- 应用激活函数：\n",
    "$$\n",
    "a^{[1](i)} = \\sigma(z^{[1](i)})\n",
    "$$\n",
    "\n",
    "**计算第二层神经元的输出**：\n",
    "- 计算线性组合：\n",
    "$$\n",
    "z^{[2](i)} = W^{[2]}(i) \\cdot a^{[1](i)} + b^{[2]}(i)\n",
    "$$\n",
    "- 应用激活函数：\n",
    "$$\n",
    "a^{[2](i)} = \\sigma(z^{[2](i)})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以向量化之后总结如下：    \n",
    "\n",
    "<img src='images/双层向量化.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （三）激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid函数有时候并不是最好的激活函数。这里介绍4种激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：下面提到的“梯度消失”指x值很大的时候斜率很小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Sigmoid 函数\n",
    "- **公式**：$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "- **求导**：$ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $\n",
    "- **图像**：   \n",
    "  <img src=\"images/sigmoid函数.png\" width=\"300\"/>\n",
    "- **使用场景**：\n",
    "  - 通常用于二分类问题中的输出层，因为它的输出范围是 (0,1)，可以表示概率。\n",
    "  - 不太适用于隐藏层，因为在深层网络中容易引起梯度消失的问题。\n",
    "\n",
    "#### 2. Tanh 函数\n",
    "- **公式**：$ \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} $\n",
    "- **求导**：$ \\tanh'(z) = 1 - \\tanh^2(z) $\n",
    "- **图像**：  \n",
    "  <img src=\"images/tanh函数.png\" width=\"300\"/>\n",
    "- **使用场景**：\n",
    "  - 常用于隐藏层，因为它的输出范围是 (-1,1)，比 sigmoid 函数的输出范围更广，可以更好地帮助模型学习。\n",
    "  - 同样存在梯度消失的问题，尤其是在深层网络中。\n",
    "\n",
    "#### 3. ReLU 函数\n",
    "- **公式**：$ \\text{ReLU}(z) = \\max(0, z) $\n",
    "- **求导**：\n",
    "  - 当 $ z > 0 $ 时, $\\text{ReLU}'(z) = 1$\n",
    "  - 当 $ z \\leq 0 $ 时, $\\text{ReLU}'(z) = 0$\n",
    "- **图像**：   \n",
    "  <img src=\"images/ReLU函数.png\" width=\"300\"/>\n",
    "- **使用场景**：\n",
    "  - 非常流行用于各种网络的隐藏层，特别是在卷积神经网络中。\n",
    "  - 有助于解决梯度消失问题，训练速度通常比 sigmoid 和 tanh 快。\n",
    "  - 但存在死神经元问题，即一旦输入小于0，ReLU 激活的导数为0，该神经元可能再也不会对任何数据有激活现象了。\n",
    "\n",
    "#### 4. Leaky ReLU 函数\n",
    "- **公式**：$ \\text{Leaky ReLU}(z) = \\max(0.01z, z) $\n",
    "- **求导**：\n",
    "  - 当 $ z > 0 $ 时, $\\text{Leaky ReLU}'(z) = 1$\n",
    "  - 当 $ z \\leq 0 $ 时, $\\text{Leaky ReLU}'(z) = 0.01$\n",
    "- **图像**：   \n",
    "  <img src=\"images/Leaky ReLU函数.png\" width=\"300\"/>\n",
    "- **使用场景**：\n",
    "  - 用于解决 ReLU 死神经元问题，允许小的梯度当 \\( z \\leq 0 \\) 时通过，提供所有神经元持续的梯度更新。\n",
    "  - 适合用于深度学习网络中，特别是在面对复杂问题时，能够提高模型的稳定性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （四）浅层神经网络的梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用$n_x$表示输入特征的个数，所以根据不同的层数,$n^{[1]}$表示隐藏层单元个数，$n^{[2]}$表示输出单元个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**神经网络权重和偏置的维度说明：**\n",
    "\n",
    "- 第一层的权重 $ W^{[1]} $ 和偏置 $ b^{[1]} $ 的维度分别为 $ (n^{[1]}, n^{[0]}) $ 和 $ (n^{[1]}, 1) $，其中 $ n^{[0]} $ 是输入层的特征数量，$ n^{[1]} $ 是第一层的单元数或神经元数。\n",
    "- 第二层的权重 $ W^{[2]} $ 和偏置 $ b^{[2]} $ 的维度分别为 $ (n^{[2]}, n^{[1]}) $ 和 $ (n^{[2]}, 1) $，其中 $ n^{[2]} $ 是第二层的单元数。\n",
    "\n",
    "**loss function**：（与第一部分完全相同）\n",
    "\n",
    "$$ \n",
    "L(\\hat{y},y) = -ylog(\\hat{y})-(1-y)log(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Cost function:**\n",
    "\n",
    "$$\n",
    "J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}, y)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation 正向传播:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   z^{[1]} = W^{[1]} x + b^{[1]}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   a^{[1]} = \\sigma(z^{[1]})\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   a^{[2]} = g(z^{[2]}) = \\sigma(z^{[2]})\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back Propagation 反向传播:**\n",
    "\n",
    "\n",
    "   $$\n",
    " \\quad dz^{[2]} = a^{[2]} - Y, \\quad Y = [y^{[1]} \\dots y^{[m]}]\n",
    "   $$\n",
    "\n",
    "   $$\n",
    " \\quad dW^{[2]} = \\frac{1}{m} dz^{[2]} (a^{[1]})^T\n",
    "   $$\n",
    "\n",
    "   $$\n",
    " \\quad db^{[2]} = \\frac{1}{m} \\text{np.sum}(dz^{[2]}, \\text{axis} = 1, \\text{keepdims} = \\text{True})\n",
    "   $$\n",
    "\n",
    "   $$\n",
    " \\quad dz^{[1]} = W^{[2]T} dz^{[2]} * g^{[1]'}(z^{[1]})\n",
    "   $$\n",
    "\n",
    "   $$\n",
    " \\quad dW^{[1]} = \\frac{1}{m} dz^{[1]} x^T\n",
    "   $$\n",
    "\n",
    "\n",
    "   $$\n",
    " \\quad db^{[1]} = \\frac{1}{m} \\text{np.sum}(dz^{[1]}, \\text{axis} = 1, \\text{keepdims} = \\text{True})\n",
    "   $$\n",
    "\n",
    "注意：\n",
    "1. $g^{[1]'}$ 表示在隐藏层中使用的激活函数的导数。\n",
    "2. axis=1表示水平相加求和\n",
    "3. keepdims=True 是为了防止出现(n,)这种，确保输出为(n,1),或者使用reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度下降：**    \n",
    "\n",
    "\n",
    "$$\n",
    "W^{[1]} = W^{[1]} - \\alpha dW^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{[1]} = b^{[1]} - \\alpha db^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W^{[2]} = W^{[2]} - \\alpha dW^{[2]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{[2]} = b^{[2]} - \\alpha db^{[2]}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：这里只给出了公式，没有给出推导，没必要**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （五）随机初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的二分类神经网络不同，多层神经网络的权重W不能都是0    \n",
    "因为如果 $W^{[1]}$ 和 $W^{[2]}$ 都是全为0的矩阵的话，就会导致这个神经网络每一个节点完全相同，完全对称，这就失去了神经网络的意义     \n",
    "所以要对W进行高斯分布的随机初始化    \n",
    "然而b是不需要的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/随机初始化.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于上面这个神经网络而言，需要随机初始化如下\n",
    "W_1=np.random.randn(2,2)*0.01\n",
    "W_2=np.random.randn(2,2)*0.01\n",
    "b_1=np.zeros((2,1))\n",
    "b_2=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要乘以0.01呢？    \n",
    "因为让权重小一点，对于隐藏层激活函数是tanh或炸sigmoid的，训练速度会快很多。   \n",
    "因为中间斜率大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (六)注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **W1，W2等是不需要再向量化到一个矩阵中的**   \n",
    "向量化的意思是：W1内部的多个同层节点的计算方法相同，所以可以保存到一个大矩阵W1中，同理推广     \n",
    "然而对于W1和W2而言，他们作为权重，具有不同的形状，不同的激活函数，不同的正反向传播算法，所以是不可以放到一个向量中的     \n",
    "对于n1，n2,还有b1,b2也是这个道理\n",
    "\n",
    "2. **关于训练集和测试集**：    \n",
    "注意训练集和测试集不能只包含猫，否则会导致它无法学习如何区分猫和非猫。这意味着你的模型实际上没有进行有效的学习来解决二分类问题，而只是学会了识别所有输入为同一类别。\n",
    "在这种情况下，会导致输出的准确率变成100%\n",
    "\n",
    "3. **关于隐藏层节点数的选择**：   \n",
    "对于较简单的问题，较少的单元可能就足够了。   \n",
    "对于复杂的问题，如复杂的图像识别或语音识别任务，可能需要更多的隐藏层和更多的单元     \n",
    "过拟合：如果隐藏层单元太多，模型可能会过度学习训练数据的细节和噪声，导致在新的或未见过的数据上表现不佳。    \n",
    "欠拟合：如果隐藏层单元太少，模型可能没有足够的能力来捕捉数据中的复杂模式，导致训练和测试性能都不佳。    \n",
    "通常需要通过实验来找到合适的层数和每层的单元数。可以开始于一个相对较小的网络，逐渐增加单元数或层数，直到测试误差不再显著下降。   \n",
    "还有一些经验法则如：   \n",
    "a. 隐藏层单元数可以设置为输入层和输出层单元数的平均值。   \n",
    "b. 可以尝试设置隐藏层单元数为输入特征数的2/3加上输出单元数。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三部分：深层神经网络（Deep Neural Networks） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （一）深层神经网络的介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/四层神经网络.png'>\n",
    "\n",
    "上图中我们看到一个典型的四层神经网络，对于每一层的节点数，可以这样表示：    \n",
    " \n",
    "$$L=4 ， n^{[0]}=n^x=3 , n^{[1]}=5 , n^{[2]} =5 , n^{[3]}=3 ， n^{[4]}=n^{[L]}=1 $$\n",
    "\n",
    "每一层l的激活函数记为 $a^{[l]}$, 其中第0层的$X=a^{[0]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图解释了深度神经网络的内在机制，浅层隐藏层会去识别边缘，中层隐藏层识别部分特征，深层隐藏层可以构建一张完整人脸    \n",
    "\n",
    "<img src='images/深层神经网络原理解释.png' width=500>\n",
    "\n",
    "这种原理对于语言识别等神经网络也是适用的，比如浅层神经元处理小的音调升降，中间神经元识别音位（比如元音辅音），深层神经元识别单词，再形成词组，句子等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，相比于浅层神经网络，想要得到相同的计算结果，深层神经网络需要的总节点数量 比浅层神经网络 要少非常非常多（尤其是对于规模比较大的计算）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (二)深度神经网络的正向和反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "对于l层而言：    \n",
    "\n",
    "**正向传播：**\n",
    "\n",
    "   \n",
    "   $$\n",
    "   Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   A^{[l]} = g^{[l]}(Z^{[l]})\n",
    "   $$\n",
    "\n",
    "其中对于第一层而言$A^{[l-1]}=A^{[0]}$就是$X$    \n",
    "\n",
    "**反向传播：**\n",
    "$$ \n",
    "dz^{[l]} = da^{[l]} \\ast g'^{[l]}(z^{[l]})  \\\\\n",
    "dw^{[l]} = dz^{[l]} \\cdot a^{[l-1]}  \\\\\n",
    "db^{[l]} = dz^{[l]}  \\\\\n",
    "da^{[l-1]} = (w^{[l]})^T \\cdot dz^{[l]}  \\\\\n",
    "$$\n",
    "向量化之后如下：\n",
    "$$\n",
    "dZ^{[l]} = dA^{[l]} \\ast g'^{[l]}(Z^{[l]})   \\\\\n",
    "dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot (A^{[l-1]})^T    \\\\\n",
    "db^{[l]} = \\frac{1}{m} \\text{np.sum}(dZ^{[l]}, \\text{axis} = 1, \\text{keepdims} = True)  \\\\\n",
    "dA^{[l-1]} = (W^{[l]})^T \\cdot dz^{[l]}   \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个计算流程如图\n",
    "\n",
    "<img src='images/深层神经网络计算机制.png' width=600>\n",
    "\n",
    "对于每一次训练，从X开始，向右正向传播，计算成本函数，然后反向传播    \n",
    "从计算过程中可以看出，反向传播中需要正向传播的相应环节传递参数$Z^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是仔细观察参数，我们可以发现有一个参数似乎没有得到计算，反向传播的第一个$dA^{[L]}$     \n",
    "我们必须先算出这个值，   由于对于最后一层而言，     \n",
    "$$ \n",
    "L(\\hat{y},y) = -ylog(\\hat{y})-(1-y)log(1-\\hat{y})\n",
    "$$\n",
    "其中$y$是真实标签，$y^hat$是预测概率，也就是$A^{[L]}$   \n",
    "所以要求的$dA^{[L]}$其实就是\n",
    "\n",
    "\n",
    "$$\n",
    " dA^{[L]}=\\frac{\\partial L}{\\partial A^{[L]}} = - \\left( \\frac{y}{A^{[L]}} - \\frac{1 - y}{1 - A^{[L]}} \\right) \n",
    "$$\n",
    "\n",
    "用python表达式就是\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一层的维度：   \n",
    "$$\n",
    "b^{[l]} : (n^{[l]},1)   \\\\\n",
    "W^{[l]} : (n^{[l]},n^{[l-1]})  \\\\\n",
    "\n",
    "$$\n",
    "\n",
    "$dW^{[l]}$和$W^{[l]}$的维度相同，$db^{[l]}$和$b^{[l]}$的维度相同\n",
    "\n",
    "对于z和a而言：    \n",
    "**向量化前：**    \n",
    "$$\n",
    "Z^{[l]} : (n^{[l]},1)   \\\\\n",
    "A^{[l]} : (n^{[l]},1)   \\\\\n",
    "$$\n",
    "**向量化后**:\n",
    "\n",
    "$$\n",
    "Z^{[l]} = (z^{[l](1)}, z^{ }, z^{ }, \\ldots, z^{[l](m)})   \\\\\n",
    "Z^{[l]} : (n^{[l]}, m)  \\\\\n",
    "A^{[l]}: (n^{[l]}, m)    \\\\\n",
    "A^{[0]} = X = (n^{[0]}, m)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （三）注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 不是只有第一个W和b需要更新，每一个W和b都要更新！这样才是每一个神经元都得到训练！\n",
    "2. 对于深度神经网络，在初始化W的时候，不要再使用*0.01来初始化权重W的标准差，这会导致神经网络在训练初期就陷入较小的梯度值（梯度消失问题）， cost会一直收敛在0.693147，也就是-log(0.5)。对于ReLU激活函数，建议使用He初始化。    He 初始化有助于神经网络更快地收敛。这是因为它避免了权重在训练初期过小或过大，这些极端值可能会导致学习过程缓慢或不稳定。\n",
    "3. 对于深度神经网络，仅仅是100张照片是远远不够的，虽然理论上可以用很少的图片（如几百张）来训练一个模型，但这通常会导致模型性能不佳或者过拟合。有些来源建议至少需要1000张图片每个类别来训练一个稳健的模型 。数据的多样性和质量同样重要。不仅需要数量足够，还需要保证数据能够覆盖到类别的各种变化，包括不同的环境、角度、光照条件等。\n",
    "4. 0.6931471就是没有成功学习的表现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四部分：深层神经网络的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (一) 训练集，验证集和测试集(train\\Dev\\test sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般把收集到的数据按照 6 : 2 ：2的比例分配。但是对于非常大体量的数据而言，验证集和测试集占比例会更小。    \n",
    "同时，应当尽可能保持 验证集和测试集 相似，防止出现问题    \n",
    "\n",
    "如果数据量不够，没有测试集也没有关系\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （二）偏差和方差（Bias\\Variance）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠拟合（underfitting）：高偏差（high bias）    \n",
    "过拟合（overfitting）： 高方差（high variance）     \n",
    "适度拟合（just right）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何判断是哪种情况呢？    \n",
    "假设最优误差是0% （最优误差又称为贝叶斯误差，是人眼识别的误差）   \n",
    "\n",
    "| 情况         | 训练集误差 | 测试集误差 | 描述         |\n",
    "|--------------|------------|------------|--------------|\n",
    "| 过拟合（高方差） | 1%         | 15%        | 过拟合，模型在训练集上表现很好但在测试集上表现较差，说明模型过于复杂，对训练数据过度敏感。 |\n",
    "| 欠拟合（高偏差） | 15%        | 16%        | 欠拟合，模型在训练集和测试集上都表现不佳，说明模型过于简单，无法捕捉基本趋势。 |\n",
    "| 高偏差+高方差  | 15%        | 30%        | 模型在训练集上的误差较高，并且在测试集上误差更大，说明模型既不能准确捕捉趋势也无法泛化到新数据。 |\n",
    "| 适度拟合      | 1%         | 2%         | 适度拟合，模型在训练集和测试集上都表现很好，误差低且相差不大，说明模型泛化能力强。 |\n",
    "\n",
    "\n",
    "总之：训练集的误差决定是否是 高偏差，测试集和训练集的准确率差异大小决定是否是 高方差    \n",
    "注意：如果最优误差不是0%，而是比如15%，那么这里的第二种情况也可以算作是 适度拟合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差和偏差是在优化算法中最基本的衡量指标：   \n",
    "\n",
    "\n",
    "第一步：判断是否高偏差（是否成功拟合）\n",
    "- **如果偏差偏高：**\n",
    "  1. 使用更深更复杂的网络\n",
    "  2. 增加训练时间\n",
    "  3. 使用更先进的优化算法\n",
    "  4. 使用NN卷积神经网络\n",
    "\n",
    "- **当偏差正常之后，进行第二步：**\n",
    "\n",
    "第二步: 判断是否高方差（是否过拟合）\n",
    "- **如果方差偏高：**\n",
    "  1. 增加数据量\n",
    "  2. 正则化（Regularization）\n",
    "  3. 使用NN卷积神经网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （三）正则化 Regulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化主要有两种：**L2正则化 和 dropout机制**  \n",
    "其实还有L1正则化，但是性能不如L2所以就不介绍了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **L2正则化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于单个节点而言，之前我们对成本函数的定义如下：    \n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "现在修改如下：\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L({\\hat y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\|w\\|_2^2 \\\\\n",
    "||w||_2^2 = \\sum_{j=1}^{n_x} w_j^2 = w^T w\n",
    "$$\n",
    "\n",
    "\n",
    "注意： \n",
    "1. $||w||_2$是向量参数w的欧几里得范数（也就是L2范数），算法如上。   $||w||_1$是L1范数，算法有点不一样，这里不讲了\n",
    "2. $\\lambda$ 是正则化参数，需要慢慢试出来，也是一个超参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络中的L2正则化：    \n",
    "$$\n",
    "J(w^{(1)}, b^{(1)}, \\ldots, w^{(L)}, b^{(L)}) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^L \\|w^{(l)}\\|_F^2 \\\\\n",
    "\\|w^{(l)}\\|_F^2 = \\sum_{i=1}^{n^{(l-1)}} \\sum_{j=1}^{n^{(l)}} (w_{ij}^{(l)})^2\n",
    "$$\n",
    "\n",
    "\n",
    "注意：   \n",
    "1. 之前讲过 $ W^{[l]} $的形状是 $(n^{[l]},n^{[l-1]})  $\n",
    "2. 该矩阵范数被称为弗罗贝尼乌斯范数，用下标F表示 \n",
    "\n",
    "\n",
    "对于L2正则化，其反向传播（BackProp）如下：     \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{\\partial L}{\\partial w} + \\frac{\\lambda}{m} w \\\\\n",
    "dW = \\text{From backprop} + \\frac{\\lambda}{m} w \\\\\n",
    "w := w - \\alpha dW\n",
    "$$\n",
    "\n",
    "\n",
    "注意:    \n",
    "1. 显而易见，W被减去的更多了，所以L2正则化又叫 **权重衰减（Weight Decay）**\n",
    "2. From backprop 是通过反向传播算法计算得到的权重的梯度,也就是上一部分中已经算出的部分\n",
    "3. $\\lambda$ 的取值方法：    \n",
    "- **小型模型和/或大型数据集**：尝试 $10^{-7}$ 到 $10^{-5}$\n",
    "- **大型模型和/或小型数据集**：尝试 $10^{-5}$ 到 $10^{-3}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dropout正则化**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout会遍历网络的每一层，并随机从这一层中消除某几个节点。每一层都要设置一个保留节点的概率（比例），各个层设置的比例可以不一样      \n",
    "这种随机“关闭”神经元的效果是，网络不能依赖于任何一个神经元，因为它可能在任何训练轮次中被关闭。     \n",
    "这迫使网络分散学习信息，从而增加其泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何实现dropout呢？我们以一个层数L=3的神经网络为例    \n",
    "首先要设计一个具体数字keep_prob,大小为0~1，表示保留某个隐藏单元的概率。      \n",
    "下面的案例中被设置为0.8，表示为每个节点有0.8的概率被保存    \n",
    "然后定义d，$d^{[3]}$表示一个3层的dropout向量    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob=0.8\n",
    "d3=np.random.rand(a3.shape[0],a3.shape[1])< keep_prob\n",
    "# d3 是一个与 a3 （某层的激活输出）同形状的布尔矩阵，其中的每个元素都是通过比较一个随机数（从均匀分布中抽取）和 keep_prob 获得的。\n",
    "# 如果随机数小于 keep_prob，相应位置的值为 True（意味着该神经元在这次迭代中保持激活），否则为 False（神经元输出被置为0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来更新a3，直接把a3和d3相乘，让$d^{[3]}$中的为0的元素和$a^{[3]}$中的对应元素归零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3=np.multiply(a3,d3)\n",
    "#python会自动把True和False转化为1和0\n",
    "\n",
    "a3/=keep_prob  # 最后向外拓展a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要除keep_prob呢？       \n",
    "因为a3中已经有20%的节点被删除了，为了不影响Z4的期望值，a3要除以0.8来修正或弥补所需的那20%, 使Z4的期望值不变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout机制一般在正向传播中使用，并且每训练一次就要dropout一次     \n",
    "但是在对应的反方向传播的每一层也要使用 对应相同的dropout mask  进行处理 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：    \n",
    "1. 当keep_prob被设置为1时，就关闭了dropout机制\n",
    "2. 在测试阶段，不需要使用dropout\n",
    "3. 对于节点数比较多的层，把keep_prob设置的小一点（多删一点），反之设置的大一点\n",
    "4. 除非算法过拟合，否则不使用 dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout机制的一大缺点是代价函数J不能被再明确定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **正则化的意义**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2正则化使部分权重被衰减，Dropout机制直接删除了部分节点，所以都是简化了网络，但是缺没有减少深度，所以防止了过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，由于sigmoid和tanh函数中间部分更接近于线性。而当W变小时，Z也会变小，所以会更接近函数的中间部分，也就是接近线性。    \n",
    "而线性层的叠加是不会增加复杂度的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （四）其他正则化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上面说到的L2正则化和Dropout之外，还有几种正则化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法一：人工图片扩增\n",
    "\n",
    "\n",
    "<img src='images/人工图片扩增.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法二：**Early Stopping**\n",
    "\n",
    "<img src='images/early stopping.png' width=500>    \n",
    "\n",
    "上图中蓝线代表训练集误差，紫线代表测试误差    \n",
    "通过及时停止训练，来防止过拟合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是这种方法有一个缺点：\n",
    "就是不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，    \n",
    "因为现在你不再尝试降低代价函数，所以代价函数J的值可能不够小，同时你又希望不出现过拟合，    \n",
    "你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping的优点是，只运行一次梯度下降，你可以找出 𝑤的较小值，中间值和较大值，而无需尝试 𝐿2正则化超级参数 𝜆的很多值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （五） 归一化输入 （Normalizing inputs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法用于加速训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设一个训练集有两个特征（输入特征为2维），归一化需要两个步骤：\n",
    "1. 把均值降到0\n",
    "2. 归一化方差\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/归一化输入.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化的目的是将输入数据转换为统一的尺度，帮助优化算法（如梯度下降）更快地收敛到最小损失\n",
    "\n",
    "\n",
    "$$\n",
    "X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "其中 $\\mu$ 是特征的均值，$\\sigma $ 是特征的标准差。  \n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "注意：\n",
    "1. 其中 m 是特征值的数量，$x_i$ 是每个特征值。     \n",
    "2. 这种转换后，特征集 X 的新均值将是0，标准差将是1，即 $X_{\\text{std}}$ 的所有值都围绕0分布，具有单位方差。\n",
    "3. 要用同样的方法调整测试集，而不是在训练集和测试集上分别预估 𝜇 和 𝜎2。  \n",
    "    也就是说测试集使用的 𝜇和 𝜎2 也是由训练集数据计算得来的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要归一化输入呢？    \n",
    "可以看下图中的代价函数3维图形（注：这些数据轴应该是 𝑤1和 𝑤2）\n",
    "\n",
    "\n",
    "<img src='images/为什么要归一化输入.png' width=500 >\n",
    "\n",
    "如上图所示，优化前必须要使用比较小的学习率（步长），但是优化后成本函数更圆，所以可以使用更大的学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "1. 这里的更快不是指计算更快。而是指可以使用更大的学习率，更快达到最优点\n",
    "2. 当 x1,x2,x3 的大小范围比较相似时，用不用归一化其实差别不大。但是如果大小范围差别很大，归一化特征值就非常重要了。\n",
    "3. 执行这类归一化并不会产生什么危害，所以可以都加上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （六）梯度消失/梯度爆炸（Vanishing/Exploding gradients）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练深度神经网络时，有时坡度会变得非常大或者非常小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度爆炸是因为在极深的神经网络中，某一个位置的权重经过反复的乘法，出现了指数爆炸的现象    \n",
    "梯度消失的原理也是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要解决这个问题，需要研究一下 **神经网络的权重初始化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络中，线性组合的计算公式为：\n",
    "$$\n",
    "Z = w_1x_1 + w_2x_2 + \\ldots + w_nx_n，b = 0，\n",
    "$$\n",
    "暂时忽略 𝑏，为了预防 𝑧值过大或过小， 所以当𝑛越大时，我们希望 𝑤𝑖越小，\n",
    "所以权重$w_i$的初始化公式为：\n",
    "$$\n",
    "w_i = \\frac{1}{n}\n",
    "$$\n",
    "其中，$n$是神经元的输入数量。\n",
    "\n",
    "在Python中，可以使用numpy库如下初始化第$[l]$层的权重：\n",
    "$$\n",
    "W[l] = np.random.randn(shape) * np.sqrt(\\frac{1}{n^{[l-1]}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**He初始化**     \n",
    "适用于使用RuLU激活函数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2. / layer_dims[l-1])  #He初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xavier初始化**     \n",
    "适用于使用tanh激活函数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1. / layer_dims[l-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五部分：算法的优化 （Optimization Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （一）Mini-batch 梯度下降（Mini-batch gradient descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们之前尝试过，对于大量的训练数据，进行一次梯度下降的时间非常长。   \n",
    "\n",
    "$X = [x^{(1)}, x^{(2)}, \\dots, x^{(m)}]$ 。   维度 $n_x \\times m$     \n",
    "$Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}]$ 。   维度 $1 \\times m$     \n",
    "\n",
    "\n",
    "现在把 X 分为很多个小集合，称为mini-batch。    \n",
    "比如把 $x^{(1)}到 x^{(100)}$ 作为一个mini-batch， 称为 $ X^{\\{1\\}}$,     \n",
    "那么如果 $X = [x^{(1)}, x^{(2)}, \\dots, x^{(1000)}]$，则就可以拆分为$ X^{\\{1\\}} $ 到 $ X^{\\{10\\}}$     \n",
    "同理，如果 $Y = [y^{(1)}, y^{(2)}, \\dots, y^{(1000)}]$，则就可以拆分为$ Y^{\\{1\\}} $ 到 $ Y^{\\{10\\}}$   \n",
    "\n",
    "所以 $ X^{\\{t\\}}$ 的大小为 $n_x \\times n$ ,  $ Y^{\\{t\\}}$ 的大小为 $1 \\times n$，    \n",
    "其中n为单个 mini-batch 的样本数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：    \n",
    "使用小括号上标表示第几个训练样本， 中括号上标表示第几层，大括号上标表示第几个 mini-batch     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
